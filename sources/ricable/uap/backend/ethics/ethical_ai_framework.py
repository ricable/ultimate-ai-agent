"""
Agent 39: Ethical AI & Algorithmic Fairness Engine
Implements comprehensive fairness metrics, bias detection,
ethical decision-making frameworks, and transparency tools.
"""

import asyncio
import json
import logging
import numpy as np
import pandas as pd
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Set, Tuple, Union, Callable
from uuid import uuid4
from enum import Enum
import torch
import torch.nn as nn
from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score
from sklearn.preprocessing import LabelEncoder
import warnings

logger = logging.getLogger(__name__)


class EthicalPrinciple(Enum):
    """Core ethical principles for AI systems"""
    FAIRNESS = "fairness"
    TRANSPARENCY = "transparency"
    ACCOUNTABILITY = "accountability"
    BENEFICENCE = "beneficence"
    NON_MALEFICENCE = "non_maleficence"
    AUTONOMY = "autonomy"
    JUSTICE = "justice"
    DIGNITY = "dignity"
    PRIVACY = "privacy"


class FairnessMetric(Enum):
    """Types of fairness metrics"""
    DEMOGRAPHIC_PARITY = "demographic_parity"
    EQUALIZED_ODDS = "equalized_odds"
    EQUALITY_OF_OPPORTUNITY = "equality_of_opportunity"
    PREDICTIVE_PARITY = "predictive_parity"
    INDIVIDUAL_FAIRNESS = "individual_fairness"
    COUNTERFACTUAL_FAIRNESS = "counterfactual_fairness"
    CAUSAL_FAIRNESS = "causal_fairness"


class BiasType(Enum):
    """Types of algorithmic bias"""
    REPRESENTATION_BIAS = "representation_bias"
    MEASUREMENT_BIAS = "measurement_bias"
    AGGREGATION_BIAS = "aggregation_bias"
    EVALUATION_BIAS = "evaluation_bias"
    DEPLOYMENT_BIAS = "deployment_bias"
    HISTORICAL_BIAS = "historical_bias"
    CONFIRMATION_BIAS = "confirmation_bias"


class SeverityLevel(Enum):
    """Severity levels for ethical violations"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


@dataclass
class ProtectedGroup:
    """Represents a protected demographic group"""
    group_id: str
    name: str
    attribute: str
    values: List[str]
    description: str
    legal_protection: bool
    intersectional_groups: List[str]


@dataclass
class FairnessAssessment:
    """Results of fairness assessment"""
    assessment_id: str
    model_id: str
    dataset_id: str
    protected_groups: List[str]
    fairness_metrics: Dict[str, float]
    bias_detected: bool
    severity_level: SeverityLevel
    detailed_analysis: Dict[str, Any]
    recommendations: List[str]
    timestamp: datetime


@dataclass
class EthicalViolation:
    """Represents an ethical violation"""
    violation_id: str
    principle_violated: EthicalPrinciple
    description: str
    severity: SeverityLevel
    context: Dict[str, Any]
    affected_groups: List[str]
    evidence: List[str]
    remediation_steps: List[str]
    status: str  # "open", "acknowledged", "remediated", "closed"
    detected_at: datetime
    resolved_at: Optional[datetime]


@dataclass
class EthicalPolicy:
    """Ethical policy configuration"""
    policy_id: str
    name: str
    principles: List[EthicalPrinciple]
    fairness_thresholds: Dict[str, float]
    bias_tolerance: Dict[BiasType, float]
    mandatory_checks: List[str]
    enforcement_level: str  # "advisory", "warning", "blocking"
    scope: List[str]  # Model types or domains this applies to
    created_at: datetime
    updated_at: datetime


class FairnessMetricsCalculator:
    """Calculates various fairness metrics"""
    
    def __init__(self):
        self.supported_metrics = {
            FairnessMetric.DEMOGRAPHIC_PARITY: self.demographic_parity,
            FairnessMetric.EQUALIZED_ODDS: self.equalized_odds,
            FairnessMetric.EQUALITY_OF_OPPORTUNITY: self.equality_of_opportunity,
            FairnessMetric.PREDICTIVE_PARITY: self.predictive_parity,
            FairnessMetric.INDIVIDUAL_FAIRNESS: self.individual_fairness,
            FairnessMetric.COUNTERFACTUAL_FAIRNESS: self.counterfactual_fairness
        }
    
    async def calculate_fairness_metrics(self, y_true: np.ndarray, y_pred: np.ndarray,
                                       sensitive_attributes: np.ndarray,
                                       metrics: List[FairnessMetric] = None) -> Dict[str, float]:
        """Calculate multiple fairness metrics"""
        if metrics is None:
            metrics = list(self.supported_metrics.keys())
        
        results = {}
        
        for metric in metrics:
            if metric in self.supported_metrics:
                try:
                    value = await self.supported_metrics[metric](y_true, y_pred, sensitive_attributes)
                    results[metric.value] = value
                except Exception as e:
                    logger.error(f"Error calculating {metric.value}: {e}")
                    results[metric.value] = None
        
        return results
    
    async def demographic_parity(self, y_true: np.ndarray, y_pred: np.ndarray,
                               sensitive_attributes: np.ndarray) -> float:
        """Calculate demographic parity (statistical parity)"""
        unique_groups = np.unique(sensitive_attributes)
        
        if len(unique_groups) < 2:
            return 1.0  # Perfect parity if only one group
        
        positive_rates = []
        for group in unique_groups:
            group_mask = sensitive_attributes == group
            group_predictions = y_pred[group_mask]
            positive_rate = np.mean(group_predictions)
            positive_rates.append(positive_rate)
        
        # Calculate parity as 1 - max difference between groups
        max_diff = max(positive_rates) - min(positive_rates)
        return 1.0 - max_diff
    
    async def equalized_odds(self, y_true: np.ndarray, y_pred: np.ndarray,
                           sensitive_attributes: np.ndarray) -> float:
        """Calculate equalized odds"""
        unique_groups = np.unique(sensitive_attributes)
        
        if len(unique_groups) < 2:
            return 1.0
        
        tpr_differences = []
        fpr_differences = []
        
        # Calculate TPR and FPR for each group
        group_tprs = []
        group_fprs = []
        
        for group in unique_groups:
            group_mask = sensitive_attributes == group
            group_true = y_true[group_mask]
            group_pred = y_pred[group_mask]
            
            if len(group_true) == 0:
                continue
            
            # Calculate TPR (True Positive Rate)
            tp = np.sum((group_true == 1) & (group_pred == 1))
            fn = np.sum((group_true == 1) & (group_pred == 0))
            tpr = tp / (tp + fn) if (tp + fn) > 0 else 0
            group_tprs.append(tpr)
            
            # Calculate FPR (False Positive Rate)
            fp = np.sum((group_true == 0) & (group_pred == 1))
            tn = np.sum((group_true == 0) & (group_pred == 0))
            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
            group_fprs.append(fpr)
        
        # Calculate differences
        if len(group_tprs) >= 2:
            tpr_diff = max(group_tprs) - min(group_tprs)
            fpr_diff = max(group_fprs) - min(group_fprs)
            
            # Return average fairness across TPR and FPR
            return 1.0 - (tpr_diff + fpr_diff) / 2
        
        return 1.0
    
    async def equality_of_opportunity(self, y_true: np.ndarray, y_pred: np.ndarray,
                                    sensitive_attributes: np.ndarray) -> float:
        """Calculate equality of opportunity (equal TPR across groups)"""
        unique_groups = np.unique(sensitive_attributes)
        
        if len(unique_groups) < 2:
            return 1.0
        
        group_tprs = []
        
        for group in unique_groups:
            group_mask = sensitive_attributes == group
            group_true = y_true[group_mask]
            group_pred = y_pred[group_mask]
            
            # Calculate TPR for positive cases
            positive_mask = group_true == 1
            if np.sum(positive_mask) > 0:
                tpr = np.mean(group_pred[positive_mask])
                group_tprs.append(tpr)
        
        if len(group_tprs) >= 2:
            tpr_diff = max(group_tprs) - min(group_tprs)
            return 1.0 - tpr_diff
        
        return 1.0
    
    async def predictive_parity(self, y_true: np.ndarray, y_pred: np.ndarray,
                              sensitive_attributes: np.ndarray) -> float:
        """Calculate predictive parity (equal PPV across groups)"""
        unique_groups = np.unique(sensitive_attributes)
        
        if len(unique_groups) < 2:
            return 1.0
        
        group_ppvs = []
        
        for group in unique_groups:
            group_mask = sensitive_attributes == group
            group_true = y_true[group_mask]
            group_pred = y_pred[group_mask]
            
            # Calculate Positive Predictive Value (Precision)
            predicted_positive_mask = group_pred == 1
            if np.sum(predicted_positive_mask) > 0:
                ppv = np.mean(group_true[predicted_positive_mask])
                group_ppvs.append(ppv)
        
        if len(group_ppvs) >= 2:
            ppv_diff = max(group_ppvs) - min(group_ppvs)
            return 1.0 - ppv_diff
        
        return 1.0
    
    async def individual_fairness(self, y_true: np.ndarray, y_pred: np.ndarray,
                                sensitive_attributes: np.ndarray,
                                distance_function: Callable = None) -> float:
        """Calculate individual fairness (similar individuals should receive similar outcomes)"""
        if distance_function is None:
            # Simple distance function based on sensitive attributes
            def default_distance(x1, x2):
                return abs(x1 - x2)
            distance_function = default_distance
        
        n = len(y_pred)
        fairness_violations = 0
        total_comparisons = 0
        
        # Sample pairs to avoid O(nÂ²) complexity
        sample_size = min(1000, n // 2)
        indices = np.random.choice(n, sample_size, replace=False)
        
        for i in range(len(indices)):
            for j in range(i + 1, len(indices)):
                idx1, idx2 = indices[i], indices[j]
                
                # Calculate distance between individuals
                distance = distance_function(sensitive_attributes[idx1], sensitive_attributes[idx2])
                
                # Calculate outcome difference
                outcome_diff = abs(y_pred[idx1] - y_pred[idx2])
                
                # Check if similar individuals (small distance) have different outcomes
                if distance < 0.1 and outcome_diff > 0.5:  # Thresholds can be tuned
                    fairness_violations += 1
                
                total_comparisons += 1
        
        if total_comparisons > 0:
            violation_rate = fairness_violations / total_comparisons
            return 1.0 - violation_rate
        
        return 1.0
    
    async def counterfactual_fairness(self, y_true: np.ndarray, y_pred: np.ndarray,
                                    sensitive_attributes: np.ndarray,
                                    counterfactual_predictions: np.ndarray = None) -> float:
        """Calculate counterfactual fairness"""
        # Simplified counterfactual fairness
        # In practice, requires causal models and counterfactual predictions
        
        if counterfactual_predictions is None:
            # Generate approximate counterfactuals by flipping sensitive attributes
            counterfactual_predictions = y_pred.copy()
            # This is a simplification - real counterfactuals require causal inference
        
        # Calculate difference between factual and counterfactual predictions
        differences = np.abs(y_pred - counterfactual_predictions)
        avg_difference = np.mean(differences)
        
        # Return fairness as 1 - average difference
        return 1.0 - min(avg_difference, 1.0)


class BiasDetector:
    """Detects various types of algorithmic bias"""
    
    def __init__(self):
        self.bias_detectors = {
            BiasType.REPRESENTATION_BIAS: self._detect_representation_bias,
            BiasType.MEASUREMENT_BIAS: self._detect_measurement_bias,
            BiasType.AGGREGATION_BIAS: self._detect_aggregation_bias,
            BiasType.EVALUATION_BIAS: self._detect_evaluation_bias,
            BiasType.HISTORICAL_BIAS: self._detect_historical_bias
        }
    
    async def detect_bias(self, data: pd.DataFrame, model_predictions: np.ndarray,
                         ground_truth: np.ndarray, sensitive_attributes: List[str],
                         bias_types: List[BiasType] = None) -> Dict[str, Any]:
        """Detect multiple types of bias"""
        if bias_types is None:
            bias_types = list(self.bias_detectors.keys())
        
        bias_results = {}
        
        for bias_type in bias_types:
            if bias_type in self.bias_detectors:
                try:
                    result = await self.bias_detectors[bias_type](
                        data, model_predictions, ground_truth, sensitive_attributes
                    )
                    bias_results[bias_type.value] = result
                except Exception as e:
                    logger.error(f"Error detecting {bias_type.value}: {e}")
                    bias_results[bias_type.value] = {"error": str(e)}
        
        return bias_results
    
    async def _detect_representation_bias(self, data: pd.DataFrame, 
                                        model_predictions: np.ndarray,
                                        ground_truth: np.ndarray,
                                        sensitive_attributes: List[str]) -> Dict[str, Any]:
        """Detect representation bias in the dataset"""
        bias_indicators = {}
        
        for attr in sensitive_attributes:
            if attr in data.columns:
                # Check distribution of sensitive attribute
                value_counts = data[attr].value_counts()
                total_samples = len(data)
                
                # Calculate representation ratios
                representation_ratios = value_counts / total_samples
                
                # Check for severe under-representation (less than 5%)
                underrepresented = representation_ratios < 0.05
                
                bias_indicators[attr] = {
                    "value_counts": value_counts.to_dict(),
                    "representation_ratios": representation_ratios.to_dict(),
                    "underrepresented_groups": value_counts[underrepresented].index.tolist(),
                    "bias_detected": len(underrepresented[underrepresented]) > 0,
                    "severity": "high" if len(underrepresented[underrepresented]) > 0 else "low"
                }
        
        return bias_indicators
    
    async def _detect_measurement_bias(self, data: pd.DataFrame,
                                     model_predictions: np.ndarray,
                                     ground_truth: np.ndarray,
                                     sensitive_attributes: List[str]) -> Dict[str, Any]:
        """Detect measurement bias (different quality of data across groups)"""
        bias_indicators = {}
        
        for attr in sensitive_attributes:
            if attr in data.columns:
                unique_groups = data[attr].unique()
                group_metrics = {}
                
                for group in unique_groups:
                    group_mask = data[attr] == group
                    group_data = data[group_mask]
                    
                    # Calculate data quality metrics
                    missing_rate = group_data.isnull().sum().sum() / (len(group_data) * len(group_data.columns))
                    
                    # Check prediction accuracy for this group
                    if len(ground_truth[group_mask]) > 0:
                        group_accuracy = accuracy_score(
                            ground_truth[group_mask], 
                            model_predictions[group_mask]
                        )
                    else:
                        group_accuracy = 0.0
                    
                    group_metrics[str(group)] = {
                        "missing_rate": missing_rate,
                        "accuracy": group_accuracy,
                        "sample_size": len(group_data)
                    }
                
                # Detect bias by comparing metrics across groups
                accuracies = [metrics["accuracy"] for metrics in group_metrics.values()]
                missing_rates = [metrics["missing_rate"] for metrics in group_metrics.values()]
                
                accuracy_diff = max(accuracies) - min(accuracies) if accuracies else 0
                missing_rate_diff = max(missing_rates) - min(missing_rates) if missing_rates else 0
                
                bias_indicators[attr] = {
                    "group_metrics": group_metrics,
                    "accuracy_difference": accuracy_diff,
                    "missing_rate_difference": missing_rate_diff,
                    "bias_detected": accuracy_diff > 0.1 or missing_rate_diff > 0.1,
                    "severity": "high" if accuracy_diff > 0.2 or missing_rate_diff > 0.2 else "medium"
                }
        
        return bias_indicators
    
    async def _detect_aggregation_bias(self, data: pd.DataFrame,
                                     model_predictions: np.ndarray,
                                     ground_truth: np.ndarray,
                                     sensitive_attributes: List[str]) -> Dict[str, Any]:
        """Detect aggregation bias (assuming one model fits all groups)"""
        bias_indicators = {}
        
        for attr in sensitive_attributes:
            if attr in data.columns:
                unique_groups = data[attr].unique()
                
                if len(unique_groups) > 1:
                    group_performances = {}
                    
                    for group in unique_groups:
                        group_mask = data[attr] == group
                        
                        if len(ground_truth[group_mask]) > 0:
                            # Calculate performance metrics for each group
                            group_acc = accuracy_score(ground_truth[group_mask], 
                                                     model_predictions[group_mask])
                            
                            # Calculate correlation between features and outcomes for each group
                            group_data = data[group_mask]
                            numeric_cols = group_data.select_dtypes(include=[np.number]).columns
                            
                            if len(numeric_cols) > 0 and len(ground_truth[group_mask]) > 1:
                                correlations = []
                                for col in numeric_cols:
                                    if col != attr:
                                        corr = np.corrcoef(group_data[col].fillna(0), 
                                                         ground_truth[group_mask])[0, 1]
                                        if not np.isnan(corr):
                                            correlations.append(abs(corr))
                                
                                avg_correlation = np.mean(correlations) if correlations else 0
                            else:
                                avg_correlation = 0
                            
                            group_performances[str(group)] = {
                                "accuracy": group_acc,
                                "avg_feature_correlation": avg_correlation,
                                "sample_size": len(group_data)
                            }
                    
                    # Check if performance varies significantly across groups
                    accuracies = [perf["accuracy"] for perf in group_performances.values()]
                    correlations = [perf["avg_feature_correlation"] for perf in group_performances.values()]
                    
                    accuracy_variance = np.var(accuracies) if len(accuracies) > 1 else 0
                    correlation_variance = np.var(correlations) if len(correlations) > 1 else 0
                    
                    bias_indicators[attr] = {
                        "group_performances": group_performances,
                        "accuracy_variance": accuracy_variance,
                        "correlation_variance": correlation_variance,
                        "bias_detected": accuracy_variance > 0.01 or correlation_variance > 0.01,
                        "severity": "high" if accuracy_variance > 0.05 else "medium"
                    }
        
        return bias_indicators
    
    async def _detect_evaluation_bias(self, data: pd.DataFrame,
                                    model_predictions: np.ndarray,
                                    ground_truth: np.ndarray,
                                    sensitive_attributes: List[str]) -> Dict[str, Any]:
        """Detect evaluation bias (using inappropriate benchmarks or metrics)"""
        bias_indicators = {}
        
        for attr in sensitive_attributes:
            if attr in data.columns:
                unique_groups = data[attr].unique()
                
                # Calculate multiple evaluation metrics for each group
                group_metrics = {}
                
                for group in unique_groups:
                    group_mask = data[attr] == group
                    
                    if len(ground_truth[group_mask]) > 0:
                        group_true = ground_truth[group_mask]
                        group_pred = model_predictions[group_mask]
                        
                        # Calculate various metrics
                        accuracy = accuracy_score(group_true, group_pred)
                        
                        # Precision, Recall, F1 for binary classification
                        if len(np.unique(group_true)) == 2:
                            tn, fp, fn, tp = confusion_matrix(group_true, group_pred).ravel()
                            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
                            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
                            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
                        else:
                            precision = recall = f1 = 0
                        
                        group_metrics[str(group)] = {
                            "accuracy": accuracy,
                            "precision": precision,
                            "recall": recall,
                            "f1_score": f1,
                            "sample_size": len(group_true)
                        }
                
                # Check for metric disagreement across groups
                metric_names = ["accuracy", "precision", "recall", "f1_score"]
                metric_variances = {}
                
                for metric in metric_names:
                    values = [metrics[metric] for metrics in group_metrics.values()]
                    metric_variances[metric] = np.var(values) if len(values) > 1 else 0
                
                max_variance = max(metric_variances.values()) if metric_variances else 0
                
                bias_indicators[attr] = {
                    "group_metrics": group_metrics,
                    "metric_variances": metric_variances,
                    "bias_detected": max_variance > 0.02,
                    "severity": "high" if max_variance > 0.1 else "medium"
                }
        
        return bias_indicators
    
    async def _detect_historical_bias(self, data: pd.DataFrame,
                                    model_predictions: np.ndarray,
                                    ground_truth: np.ndarray,
                                    sensitive_attributes: List[str]) -> Dict[str, Any]:
        """Detect historical bias (perpetuating past discrimination)"""
        bias_indicators = {}
        
        for attr in sensitive_attributes:
            if attr in data.columns:
                # Check if the model perpetuates historical patterns
                unique_groups = data[attr].unique()
                
                # Compare historical outcomes vs predicted outcomes
                group_analysis = {}
                
                for group in unique_groups:
                    group_mask = data[attr] == group
                    
                    if len(ground_truth[group_mask]) > 0:
                        # Historical positive rate (from ground truth)
                        historical_positive_rate = np.mean(ground_truth[group_mask])
                        
                        # Predicted positive rate
                        predicted_positive_rate = np.mean(model_predictions[group_mask])
                        
                        # Check if model amplifies historical disparities
                        amplification_factor = predicted_positive_rate / historical_positive_rate if historical_positive_rate > 0 else 1
                        
                        group_analysis[str(group)] = {
                            "historical_positive_rate": historical_positive_rate,
                            "predicted_positive_rate": predicted_positive_rate,
                            "amplification_factor": amplification_factor,
                            "sample_size": len(data[group_mask])
                        }
                
                # Check for concerning amplification patterns
                amplification_factors = [analysis["amplification_factor"] for analysis in group_analysis.values()]
                amplification_variance = np.var(amplification_factors) if len(amplification_factors) > 1 else 0
                
                # Detect if some groups are being systematically disadvantaged
                min_amplification = min(amplification_factors) if amplification_factors else 1
                max_amplification = max(amplification_factors) if amplification_factors else 1
                amplification_ratio = max_amplification / min_amplification if min_amplification > 0 else 1
                
                bias_indicators[attr] = {
                    "group_analysis": group_analysis,
                    "amplification_variance": amplification_variance,
                    "amplification_ratio": amplification_ratio,
                    "bias_detected": amplification_ratio > 1.5 or amplification_variance > 0.1,
                    "severity": "critical" if amplification_ratio > 2.0 else "high"
                }
        
        return bias_indicators


class EthicalDecisionFramework:
    """Framework for making ethical decisions in AI systems"""
    
    def __init__(self):
        self.ethical_rules = {}
        self.decision_history = []
        self.value_weights = {
            EthicalPrinciple.FAIRNESS: 0.2,
            EthicalPrinciple.TRANSPARENCY: 0.15,
            EthicalPrinciple.ACCOUNTABILITY: 0.15,
            EthicalPrinciple.BENEFICENCE: 0.15,
            EthicalPrinciple.NON_MALEFICENCE: 0.15,
            EthicalPrinciple.AUTONOMY: 0.1,
            EthicalPrinciple.JUSTICE: 0.1
        }
    
    async def make_ethical_decision(self, decision_context: Dict[str, Any],
                                  alternatives: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Make an ethical decision given context and alternatives"""
        decision_id = str(uuid4())
        
        # Evaluate each alternative against ethical principles
        evaluations = []
        
        for i, alternative in enumerate(alternatives):
            evaluation = await self._evaluate_alternative(alternative, decision_context)
            evaluation["alternative_id"] = i
            evaluations.append(evaluation)
        
        # Select the most ethical alternative
        best_alternative = max(evaluations, key=lambda x: x["total_score"])
        
        # Create decision record
        decision_record = {
            "decision_id": decision_id,
            "context": decision_context,
            "alternatives": alternatives,
            "evaluations": evaluations,
            "selected_alternative": best_alternative["alternative_id"],
            "ethical_justification": best_alternative["justification"],
            "timestamp": datetime.utcnow(),
            "total_score": best_alternative["total_score"]
        }
        
        self.decision_history.append(decision_record)
        
        return decision_record
    
    async def _evaluate_alternative(self, alternative: Dict[str, Any],
                                  context: Dict[str, Any]) -> Dict[str, Any]:
        """Evaluate an alternative against ethical principles"""
        scores = {}
        justifications = {}
        
        # Evaluate each ethical principle
        for principle in EthicalPrinciple:
            score, justification = await self._score_principle(alternative, context, principle)
            scores[principle.value] = score
            justifications[principle.value] = justification
        
        # Calculate weighted total score
        total_score = sum(
            scores[principle.value] * self.value_weights[principle]
            for principle in EthicalPrinciple
        )
        
        return {
            "scores": scores,
            "justifications": justifications,
            "total_score": total_score,
            "justification": self._generate_overall_justification(scores, justifications)
        }
    
    async def _score_principle(self, alternative: Dict[str, Any],
                             context: Dict[str, Any],
                             principle: EthicalPrinciple) -> Tuple[float, str]:
        """Score an alternative against a specific ethical principle"""
        
        if principle == EthicalPrinciple.FAIRNESS:
            # Check if alternative promotes fairness
            fairness_indicators = alternative.get("fairness_metrics", {})
            if fairness_indicators:
                avg_fairness = np.mean(list(fairness_indicators.values()))
                score = avg_fairness
                justification = f"Fairness score based on metrics: {avg_fairness:.3f}"
            else:
                score = 0.5  # Neutral if no fairness data
                justification = "No fairness metrics available"
        
        elif principle == EthicalPrinciple.TRANSPARENCY:
            # Check transparency features
            explainability = alternative.get("explainability", 0.5)
            interpretability = alternative.get("interpretability", 0.5)
            score = (explainability + interpretability) / 2
            justification = f"Transparency based on explainability ({explainability:.2f}) and interpretability ({interpretability:.2f})"
        
        elif principle == EthicalPrinciple.ACCOUNTABILITY:
            # Check accountability mechanisms
            audit_trail = alternative.get("audit_trail", False)
            monitoring = alternative.get("monitoring_enabled", False)
            reversibility = alternative.get("reversible_decisions", False)
            
            score = (audit_trail + monitoring + reversibility) / 3
            justification = f"Accountability score based on audit trail, monitoring, and reversibility"
        
        elif principle == EthicalPrinciple.BENEFICENCE:
            # Check positive impact
            positive_impact = alternative.get("positive_impact_score", 0.5)
            score = positive_impact
            justification = f"Beneficence score: {positive_impact:.3f}"
        
        elif principle == EthicalPrinciple.NON_MALEFICENCE:
            # Check for potential harm
            harm_risk = alternative.get("harm_risk_score", 0.5)
            score = 1.0 - harm_risk  # Invert risk to get benefit score
            justification = f"Non-maleficence score (1 - harm risk): {score:.3f}"
        
        elif principle == EthicalPrinciple.AUTONOMY:
            # Check if alternative respects human autonomy
            user_control = alternative.get("user_control", 0.5)
            consent_mechanism = alternative.get("consent_mechanism", False)
            score = (user_control + consent_mechanism) / 2
            justification = f"Autonomy score based on user control and consent mechanisms"
        
        elif principle == EthicalPrinciple.JUSTICE:
            # Check distributive and procedural justice
            equal_access = alternative.get("equal_access", 0.5)
            fair_process = alternative.get("fair_process", 0.5)
            score = (equal_access + fair_process) / 2
            justification = f"Justice score based on equal access and fair process"
        
        else:
            score = 0.5
            justification = "Default neutral score for unknown principle"
        
        return score, justification
    
    def _generate_overall_justification(self, scores: Dict[str, float],
                                      justifications: Dict[str, str]) -> str:
        """Generate overall ethical justification"""
        # Find the strongest and weakest principles
        sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        strongest = sorted_scores[0]
        weakest = sorted_scores[-1]
        
        justification = f"This alternative scores highest on {strongest[0]} ({strongest[1]:.3f}) "
        justification += f"and lowest on {weakest[0]} ({weakest[1]:.3f}). "
        justification += f"Overall ethical score: {np.mean(list(scores.values())):.3f}"
        
        return justification
    
    async def update_value_weights(self, new_weights: Dict[EthicalPrinciple, float]):
        """Update the weights for ethical principles"""
        # Normalize weights to sum to 1
        total_weight = sum(new_weights.values())
        if total_weight > 0:
            self.value_weights = {
                principle: weight / total_weight
                for principle, weight in new_weights.items()
            }


class TransparencyEngine:
    """Provides transparency and explainability for AI decisions"""
    
    def __init__(self):
        self.explanation_cache = {}
        self.transparency_reports = []
    
    async def generate_explanation(self, model_output: Any, input_data: Dict[str, Any],
                                 model_info: Dict[str, Any] = None) -> Dict[str, Any]:
        """Generate explanation for a model decision"""
        explanation_id = str(uuid4())
        
        explanation = {
            "explanation_id": explanation_id,
            "model_output": model_output,
            "input_summary": self._summarize_input(input_data),
            "feature_importance": await self._calculate_feature_importance(input_data, model_info),
            "decision_path": await self._trace_decision_path(input_data, model_info),
            "confidence_level": await self._calculate_confidence(model_output, model_info),
            "alternative_scenarios": await self._generate_counterfactuals(input_data, model_info),
            "plain_language_explanation": await self._generate_plain_language_explanation(
                model_output, input_data, model_info
            ),
            "timestamp": datetime.utcnow()
        }
        
        self.explanation_cache[explanation_id] = explanation
        return explanation
    
    async def generate_transparency_report(self, model_id: str, 
                                         time_period: Tuple[datetime, datetime]) -> Dict[str, Any]:
        """Generate comprehensive transparency report"""
        report_id = str(uuid4())
        start_date, end_date = time_period
        
        report = {
            "report_id": report_id,
            "model_id": model_id,
            "period": {
                "start_date": start_date.isoformat(),
                "end_date": end_date.isoformat()
            },
            "model_performance": await self._analyze_model_performance(model_id, time_period),
            "decision_patterns": await self._analyze_decision_patterns(model_id, time_period),
            "fairness_analysis": await self._analyze_fairness_over_time(model_id, time_period),
            "user_interactions": await self._analyze_user_interactions(model_id, time_period),
            "system_changes": await self._track_system_changes(model_id, time_period),
            "recommendations": await self._generate_improvement_recommendations(model_id),
            "generated_at": datetime.utcnow()
        }
        
        self.transparency_reports.append(report)
        return report
    
    def _summarize_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Summarize input data for explanation"""
        summary = {
            "num_features": len(input_data),
            "feature_types": {},
            "key_features": {}
        }
        
        for key, value in input_data.items():
            if isinstance(value, (int, float)):
                summary["feature_types"][key] = "numeric"
                summary["key_features"][key] = value
            elif isinstance(value, str):
                summary["feature_types"][key] = "categorical"
                summary["key_features"][key] = value
            elif isinstance(value, (list, np.ndarray)):
                summary["feature_types"][key] = "array"
                summary["key_features"][key] = f"Array of length {len(value)}"
        
        return summary
    
    async def _calculate_feature_importance(self, input_data: Dict[str, Any],
                                          model_info: Dict[str, Any] = None) -> Dict[str, float]:
        """Calculate feature importance for the decision"""
        # Simplified feature importance calculation
        importance_scores = {}
        
        for key, value in input_data.items():
            if isinstance(value, (int, float)):
                # Simple heuristic: higher absolute values are more important
                importance_scores[key] = min(abs(value) / 100.0, 1.0)
            else:
                # Default importance for non-numeric features
                importance_scores[key] = 0.5
        
        # Normalize importance scores
        total_importance = sum(importance_scores.values())
        if total_importance > 0:
            importance_scores = {
                k: v / total_importance for k, v in importance_scores.items()
            }
        
        return importance_scores
    
    async def _trace_decision_path(self, input_data: Dict[str, Any],
                                 model_info: Dict[str, Any] = None) -> List[str]:
        """Trace the decision path through the model"""
        # Simplified decision path
        path = [
            "Input data received and validated",
            "Feature preprocessing applied",
            "Model inference executed",
            "Output generated and post-processed"
        ]
        
        return path
    
    async def _calculate_confidence(self, model_output: Any,
                                  model_info: Dict[str, Any] = None) -> float:
        """Calculate confidence level for the decision"""
        # Simple confidence calculation
        if isinstance(model_output, (list, np.ndarray)):
            if len(model_output) > 1:
                # For classification, use max probability
                return float(max(model_output))
            else:
                return 0.8  # Default confidence
        else:
            return 0.8  # Default confidence for other outputs
    
    async def _generate_counterfactuals(self, input_data: Dict[str, Any],
                                      model_info: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """Generate counterfactual scenarios"""
        counterfactuals = []
        
        # Generate simple counterfactuals by modifying input features
        for key, value in input_data.items():
            if isinstance(value, (int, float)):
                # Create counterfactual by changing numeric value
                modified_input = input_data.copy()
                modified_input[key] = value * 1.1  # 10% increase
                
                counterfactual = {
                    "description": f"If {key} were {modified_input[key]:.2f} instead of {value:.2f}",
                    "modified_features": {key: modified_input[key]},
                    "predicted_outcome": "Different outcome expected"  # Would run actual prediction
                }
                counterfactuals.append(counterfactual)
        
        return counterfactuals[:3]  # Return top 3 counterfactuals
    
    async def _generate_plain_language_explanation(self, model_output: Any,
                                                 input_data: Dict[str, Any],
                                                 model_info: Dict[str, Any] = None) -> str:
        """Generate plain language explanation"""
        explanation = "Based on the provided input, the AI system made this decision by analyzing "
        explanation += f"{len(input_data)} different factors. "
        
        # Find most important features (simplified)
        numeric_features = {k: v for k, v in input_data.items() if isinstance(v, (int, float))}
        if numeric_features:
            most_important = max(numeric_features.items(), key=lambda x: abs(x[1]))
            explanation += f"The most influential factor was '{most_important[0]}' with a value of {most_important[1]}. "
        
        explanation += "The system is designed to be fair and unbiased in its decision-making process."
        
        return explanation
    
    async def _analyze_model_performance(self, model_id: str,
                                       time_period: Tuple[datetime, datetime]) -> Dict[str, Any]:
        """Analyze model performance over time period"""
        return {
            "accuracy_trend": "stable",
            "prediction_count": 1000,
            "error_rate": 0.05,
            "performance_changes": []
        }
    
    async def _analyze_decision_patterns(self, model_id: str,
                                       time_period: Tuple[datetime, datetime]) -> Dict[str, Any]:
        """Analyze decision patterns"""
        return {
            "common_decision_types": ["type_a", "type_b"],
            "decision_distribution": {"positive": 0.6, "negative": 0.4},
            "pattern_changes": []
        }
    
    async def _analyze_fairness_over_time(self, model_id: str,
                                        time_period: Tuple[datetime, datetime]) -> Dict[str, Any]:
        """Analyze fairness metrics over time"""
        return {
            "fairness_trend": "improving",
            "bias_incidents": 0,
            "group_performance_gaps": 0.02
        }
    
    async def _analyze_user_interactions(self, model_id: str,
                                       time_period: Tuple[datetime, datetime]) -> Dict[str, Any]:
        """Analyze user interactions with the system"""
        return {
            "total_interactions": 500,
            "user_feedback_sentiment": "positive",
            "explanation_requests": 50
        }
    
    async def _track_system_changes(self, model_id: str,
                                  time_period: Tuple[datetime, datetime]) -> List[Dict[str, Any]]:
        """Track system changes during the period"""
        return [
            {
                "timestamp": datetime.utcnow().isoformat(),
                "change_type": "model_update",
                "description": "Minor parameter adjustment"
            }
        ]
    
    async def _generate_improvement_recommendations(self, model_id: str) -> List[str]:
        """Generate recommendations for system improvement"""
        return [
            "Continue monitoring fairness metrics",
            "Increase transparency in decision explanations",
            "Collect more diverse training data"
        ]


class EthicalAIEngine:
    """Main engine coordinating all ethical AI components"""
    
    def __init__(self):
        self.fairness_calculator = FairnessMetricsCalculator()
        self.bias_detector = BiasDetector()
        self.ethical_framework = EthicalDecisionFramework()
        self.transparency_engine = TransparencyEngine()
        
        # Configuration and state
        self.policies: Dict[str, EthicalPolicy] = {}
        self.protected_groups: Dict[str, ProtectedGroup] = {}
        self.assessments: List[FairnessAssessment] = []
        self.violations: List[EthicalViolation] = []
        
        # Monitoring and alerting
        self.monitoring_enabled = True
        self.alert_thresholds = {
            "fairness_violation": 0.1,
            "bias_detection": 0.2,
            "ethical_score": 0.6
        }
    
    async def initialize(self) -> bool:
        """Initialize the ethical AI engine"""
        logger.info("Initializing Ethical AI & Algorithmic Fairness Engine")
        
        # Set up default policies and protected groups
        await self._setup_default_configuration()
        
        return True
    
    async def assess_model_fairness(self, model_id: str, dataset_id: str,
                                  y_true: np.ndarray, y_pred: np.ndarray,
                                  sensitive_attributes: Dict[str, np.ndarray],
                                  data: pd.DataFrame = None) -> FairnessAssessment:
        """Comprehensive fairness assessment of a model"""
        assessment_id = str(uuid4())
        
        # Calculate fairness metrics
        all_fairness_metrics = {}
        
        for attr_name, attr_values in sensitive_attributes.items():
            attr_metrics = await self.fairness_calculator.calculate_fairness_metrics(
                y_true, y_pred, attr_values
            )
            all_fairness_metrics[attr_name] = attr_metrics
        
        # Detect bias
        bias_results = {}
        if data is not None:
            bias_results = await self.bias_detector.detect_bias(
                data, y_pred, y_true, list(sensitive_attributes.keys())
            )
        
        # Determine overall bias status and severity
        bias_detected, severity = self._assess_overall_bias(all_fairness_metrics, bias_results)
        
        # Generate recommendations
        recommendations = await self._generate_fairness_recommendations(
            all_fairness_metrics, bias_results
        )
        
        # Create assessment
        assessment = FairnessAssessment(
            assessment_id=assessment_id,
            model_id=model_id,
            dataset_id=dataset_id,
            protected_groups=list(sensitive_attributes.keys()),
            fairness_metrics=all_fairness_metrics,
            bias_detected=bias_detected,
            severity_level=severity,
            detailed_analysis={
                "bias_analysis": bias_results,
                "metric_summary": self._summarize_fairness_metrics(all_fairness_metrics),
                "group_analysis": self._analyze_group_performance(y_true, y_pred, sensitive_attributes)
            },
            recommendations=recommendations,
            timestamp=datetime.utcnow()
        )
        
        self.assessments.append(assessment)
        
        # Check for violations and trigger alerts
        if bias_detected and severity in [SeverityLevel.HIGH, SeverityLevel.CRITICAL]:
            await self._handle_fairness_violation(assessment)
        
        return assessment
    
    async def make_ethical_decision(self, decision_context: Dict[str, Any],
                                  alternatives: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Make an ethical decision using the ethical framework"""
        decision = await self.ethical_framework.make_ethical_decision(
            decision_context, alternatives
        )
        
        # Log decision for audit trail
        await self._log_ethical_decision(decision)
        
        return decision
    
    async def explain_decision(self, model_output: Any, input_data: Dict[str, Any],
                             model_info: Dict[str, Any] = None) -> Dict[str, Any]:
        """Generate explanation for a model decision"""
        explanation = await self.transparency_engine.generate_explanation(
            model_output, input_data, model_info
        )
        
        return explanation
    
    async def monitor_system_ethics(self, model_id: str) -> Dict[str, Any]:
        """Continuously monitor system for ethical violations"""
        monitoring_report = {
            "model_id": model_id,
            "monitoring_timestamp": datetime.utcnow(),
            "fairness_status": "monitoring",
            "recent_assessments": [],
            "active_violations": [],
            "recommendations": []
        }
        
        # Get recent assessments for this model
        recent_assessments = [
            assessment for assessment in self.assessments[-10:]  # Last 10
            if assessment.model_id == model_id
        ]
        
        monitoring_report["recent_assessments"] = [
            {
                "assessment_id": assessment.assessment_id,
                "timestamp": assessment.timestamp.isoformat(),
                "bias_detected": assessment.bias_detected,
                "severity": assessment.severity_level.value
            }
            for assessment in recent_assessments
        ]
        
        # Check for active violations
        active_violations = [
            violation for violation in self.violations
            if violation.status == "open" and model_id in violation.context.get("model_ids", [])
        ]
        
        monitoring_report["active_violations"] = [
            {
                "violation_id": violation.violation_id,
                "principle": violation.principle_violated.value,
                "severity": violation.severity.value,
                "description": violation.description
            }
            for violation in active_violations
        ]
        
        # Generate monitoring recommendations
        if recent_assessments:
            latest_assessment = recent_assessments[-1]
            monitoring_report["recommendations"] = latest_assessment.recommendations
        
        return monitoring_report
    
    def _assess_overall_bias(self, fairness_metrics: Dict[str, Dict[str, float]],
                           bias_results: Dict[str, Any]) -> Tuple[bool, SeverityLevel]:
        """Assess overall bias status and severity"""
        bias_detected = False
        max_severity = SeverityLevel.LOW
        
        # Check fairness metrics
        for attr_metrics in fairness_metrics.values():
            for metric_value in attr_metrics.values():
                if metric_value is not None and metric_value < 0.8:  # Threshold for fairness
                    bias_detected = True
                    if metric_value < 0.5:
                        max_severity = SeverityLevel.HIGH
                    elif metric_value < 0.7:
                        max_severity = max(max_severity, SeverityLevel.MEDIUM)
        
        # Check bias detection results
        for bias_type_results in bias_results.values():
            if isinstance(bias_type_results, dict):
                for attr_results in bias_type_results.values():
                    if isinstance(attr_results, dict) and attr_results.get("bias_detected", False):
                        bias_detected = True
                        severity_str = attr_results.get("severity", "low")
                        if severity_str == "critical":
                            max_severity = SeverityLevel.CRITICAL
                        elif severity_str == "high":
                            max_severity = max(max_severity, SeverityLevel.HIGH)
                        elif severity_str == "medium":
                            max_severity = max(max_severity, SeverityLevel.MEDIUM)
        
        return bias_detected, max_severity
    
    def _summarize_fairness_metrics(self, fairness_metrics: Dict[str, Dict[str, float]]) -> Dict[str, Any]:
        """Summarize fairness metrics across all attributes"""
        summary = {
            "overall_fairness_score": 0.0,
            "worst_performing_metric": None,
            "best_performing_metric": None,
            "metrics_below_threshold": []
        }
        
        all_scores = []
        metric_scores = {}
        
        for attr, metrics in fairness_metrics.items():
            for metric_name, score in metrics.items():
                if score is not None:
                    all_scores.append(score)
                    key = f"{attr}_{metric_name}"
                    metric_scores[key] = score
                    
                    if score < 0.8:  # Threshold
                        summary["metrics_below_threshold"].append({
                            "attribute": attr,
                            "metric": metric_name,
                            "score": score
                        })
        
        if all_scores:
            summary["overall_fairness_score"] = np.mean(all_scores)
        
        if metric_scores:
            worst_key = min(metric_scores.items(), key=lambda x: x[1])
            best_key = max(metric_scores.items(), key=lambda x: x[1])
            
            summary["worst_performing_metric"] = {
                "metric": worst_key[0],
                "score": worst_key[1]
            }
            summary["best_performing_metric"] = {
                "metric": best_key[0],
                "score": best_key[1]
            }
        
        return summary
    
    def _analyze_group_performance(self, y_true: np.ndarray, y_pred: np.ndarray,
                                 sensitive_attributes: Dict[str, np.ndarray]) -> Dict[str, Any]:
        """Analyze performance across different groups"""
        group_analysis = {}
        
        for attr_name, attr_values in sensitive_attributes.items():
            unique_groups = np.unique(attr_values)
            group_performance = {}
            
            for group in unique_groups:
                group_mask = attr_values == group
                if len(y_true[group_mask]) > 0:
                    group_accuracy = accuracy_score(y_true[group_mask], y_pred[group_mask])
                    group_size = len(y_true[group_mask])
                    
                    group_performance[str(group)] = {
                        "accuracy": group_accuracy,
                        "sample_size": group_size,
                        "proportion": group_size / len(y_true)
                    }
            
            group_analysis[attr_name] = group_performance
        
        return group_analysis
    
    async def _generate_fairness_recommendations(self, fairness_metrics: Dict[str, Dict[str, float]],
                                               bias_results: Dict[str, Any]) -> List[str]:
        """Generate recommendations based on fairness assessment"""
        recommendations = []
        
        # Analyze fairness metrics
        for attr, metrics in fairness_metrics.items():
            for metric_name, score in metrics.items():
                if score is not None and score < 0.8:
                    recommendations.append(
                        f"Improve {metric_name} for {attr} (current score: {score:.3f})"
                    )
        
        # Analyze bias detection results
        for bias_type, results in bias_results.items():
            if isinstance(results, dict):
                for attr, attr_results in results.items():
                    if isinstance(attr_results, dict) and attr_results.get("bias_detected", False):
                        recommendations.append(
                            f"Address {bias_type} detected in {attr} attribute"
                        )
        
        # Add general recommendations
        if not recommendations:
            recommendations.append("Continue monitoring for fairness and bias")
        else:
            recommendations.extend([
                "Collect more diverse training data",
                "Implement bias mitigation techniques",
                "Regular fairness audits recommended"
            ])
        
        return recommendations
    
    async def _handle_fairness_violation(self, assessment: FairnessAssessment):
        """Handle detected fairness violation"""
        violation = EthicalViolation(
            violation_id=str(uuid4()),
            principle_violated=EthicalPrinciple.FAIRNESS,
            description=f"Fairness violation detected in model {assessment.model_id}",
            severity=assessment.severity_level,
            context={
                "assessment_id": assessment.assessment_id,
                "model_ids": [assessment.model_id],
                "affected_attributes": assessment.protected_groups
            },
            affected_groups=assessment.protected_groups,
            evidence=[f"Fairness assessment {assessment.assessment_id}"],
            remediation_steps=assessment.recommendations,
            status="open",
            detected_at=datetime.utcnow(),
            resolved_at=None
        )
        
        self.violations.append(violation)
        
        # Log violation
        logger.warning(f"Fairness violation detected: {violation.violation_id}")
        
        # Could trigger alerts, notifications, or automatic remediation
    
    async def _log_ethical_decision(self, decision: Dict[str, Any]):
        """Log ethical decision for audit purposes"""
        # In a production system, this would store to audit database
        logger.info(f"Ethical decision made: {decision['decision_id']}")
    
    async def _setup_default_configuration(self):
        """Set up default ethical policies and protected groups"""
        # Default ethical policy
        default_policy = EthicalPolicy(
            policy_id="default_ethical_policy",
            name="Default Ethical AI Policy",
            principles=[
                EthicalPrinciple.FAIRNESS,
                EthicalPrinciple.TRANSPARENCY,
                EthicalPrinciple.ACCOUNTABILITY
            ],
            fairness_thresholds={
                "demographic_parity": 0.8,
                "equalized_odds": 0.8,
                "equality_of_opportunity": 0.8
            },
            bias_tolerance={
                BiasType.REPRESENTATION_BIAS: 0.1,
                BiasType.MEASUREMENT_BIAS: 0.15,
                BiasType.HISTORICAL_BIAS: 0.2
            },
            mandatory_checks=["fairness_assessment", "bias_detection"],
            enforcement_level="warning",
            scope=["all_models"],
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow()
        )
        
        self.policies[default_policy.policy_id] = default_policy
        
        # Default protected groups
        protected_groups = [
            ProtectedGroup(
                group_id="gender",
                name="Gender",
                attribute="gender",
                values=["male", "female", "other"],
                description="Gender identity protection",
                legal_protection=True,
                intersectional_groups=["race", "age"]
            ),
            ProtectedGroup(
                group_id="race",
                name="Race/Ethnicity",
                attribute="race",
                values=["white", "black", "hispanic", "asian", "other"],
                description="Racial and ethnic protection",
                legal_protection=True,
                intersectional_groups=["gender", "age"]
            ),
            ProtectedGroup(
                group_id="age",
                name="Age Group",
                attribute="age_group",
                values=["young", "middle", "senior"],
                description="Age-based protection",
                legal_protection=True,
                intersectional_groups=["gender", "race"]
            )
        ]
        
        for group in protected_groups:
            self.protected_groups[group.group_id] = group
    
    async def get_system_status(self) -> Dict[str, Any]:
        """Get comprehensive system status"""
        return {
            "ethical_engine": {
                "policies_count": len(self.policies),
                "protected_groups_count": len(self.protected_groups),
                "assessments_count": len(self.assessments),
                "violations_count": len(self.violations),
                "monitoring_enabled": self.monitoring_enabled
            },
            "recent_activity": {
                "recent_assessments": len([a for a in self.assessments 
                                         if a.timestamp > datetime.utcnow() - timedelta(days=7)]),
                "open_violations": len([v for v in self.violations if v.status == "open"]),
                "critical_violations": len([v for v in self.violations 
                                          if v.severity == SeverityLevel.CRITICAL and v.status == "open"])
            },
            "fairness_metrics": {
                "supported_metrics": list(self.fairness_calculator.supported_metrics.keys()),
                "bias_detectors": list(self.bias_detector.bias_detectors.keys())
            },
            "transparency": {
                "explanations_generated": len(self.transparency_engine.explanation_cache),
                "reports_generated": len(self.transparency_engine.transparency_reports)
            }
        }