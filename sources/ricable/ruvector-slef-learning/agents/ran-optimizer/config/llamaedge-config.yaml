# LlamaEdge Configuration for RAN Optimizer LLM Agent
# This configuration defines the inference settings for the WASI-NN GGML backend

# Server Configuration
server:
  host: "0.0.0.0"
  port: 8080
  # Enable CORS for API access
  cors: true
  # API endpoints
  endpoints:
    - /v1/chat/completions
    - /v1/completions
    - /v1/embeddings
    - /v1/models

# Model Configuration
model:
  # Default model (can be overridden via --nn-preload)
  name: "ran-optimizer"
  # Supported model formats: GGUF (llama.cpp quantized)
  format: "gguf"
  # Recommended models for RAN optimization (in order of preference)
  recommended:
    # Qwen2.5 - Best for structured output and reasoning
    - name: "qwen2.5-7b-instruct"
      url: "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF/resolve/main/qwen2.5-7b-instruct-q5_k_m.gguf"
      size_gb: 5.1
      context: 32768

    # Llama 3.2 - Good balance of speed and quality
    - name: "llama-3.2-3b-instruct"
      url: "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf"
      size_gb: 2.3
      context: 131072

    # Phi-4 - Smaller model for edge deployment
    - name: "phi-4-mini"
      url: "https://huggingface.co/microsoft/Phi-4-mini-GGUF/resolve/main/phi-4-mini-q4_k_m.gguf"
      size_gb: 2.5
      context: 16384

    # Mistral - Fast inference
    - name: "mistral-7b-instruct-v0.3"
      url: "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/mistral-7b-instruct-v0.3.Q5_K_M.gguf"
      size_gb: 5.0
      context: 32768

# Inference Parameters (optimized for RAN optimization tasks)
inference:
  # Context window size
  ctx_size: 8192
  # Maximum output tokens
  n_predict: 2048
  # GPU layers (set to 0 for CPU-only)
  n_gpu_layers: 35
  # Batch size for parallel processing
  batch_size: 512
  # Number of threads (CPU)
  threads: 4

  # Sampling parameters (conservative for deterministic RAN decisions)
  temperature: 0.3
  top_p: 0.9
  top_k: 40
  repeat_penalty: 1.1
  presence_penalty: 0.0
  frequency_penalty: 0.0

  # Output control
  stream: true
  stream_stdout: true

  # GBNF grammar for structured JSON output (optional)
  grammar: |
    root ::= object
    object ::= "{" ws members ws "}"
    members ::= pair ("," ws pair)*
    pair ::= string ":" ws value
    string ::= "\"" [^"\\]* "\""
    value ::= string | number | object | array | "true" | "false" | "null"
    array ::= "[" ws (value ("," ws value)*)? ws "]"
    number ::= "-"? [0-9]+ ("." [0-9]+)?
    ws ::= [ \t\n\r]*

# RAN-Specific Configuration
ran_optimizer:
  # Optimization modes
  modes:
    - coverage
    - capacity
    - interference
    - energy
    - mobility
    - qos
    - anomaly

  # Default system prompt mode
  default_mode: "coverage"

  # Enable JSON output validation
  validate_json: true

  # Maximum retries for invalid JSON
  max_retries: 3

  # KPI thresholds for automatic recommendations
  thresholds:
    rsrp_poor: -110  # dBm
    rsrq_poor: -15   # dB
    sinr_poor: 5     # dB
    prb_high: 85     # %
    ho_sr_low: 95    # %
    drop_rate_high: 1  # %

# Logging
logging:
  level: "info"
  format: "json"
  # Log inference metadata
  log_metadata: true
  # Log input/output tokens
  log_tokens: false

# Health Checks
health:
  enabled: true
  endpoint: "/health"
  # Check interval in seconds
  interval: 30
  # Timeout for health check
  timeout: 5

# Metrics
metrics:
  enabled: true
  endpoint: "/metrics"
  # Prometheus format
  format: "prometheus"
