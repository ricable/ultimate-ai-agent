# Comprehensive 8B Model Benchmark Report
        
**Generated:** 2025-06-20T14:09:46.550204  
**System:** Darwin 25.0.0 (16 cores, 128 GB RAM)

## Executive Summary

This comprehensive benchmark evaluates available 8B parameter language models across multiple frameworks (MLX, HuggingFace) on Apple Silicon hardware. The analysis includes inference performance, memory usage, fine-tuning capabilities, and practical recommendations.

### Key Findings

- **Models Tested:** 3
- **Successful Tests:** 3
- **Frameworks:** mlx, huggingface

## Performance Comparison

### üèÜ Best Performers

| Metric | Model | Value | Framework |
|--------|--------|--------|-----------|
| **Fastest Load** | Meta-Llama-3.1-8B-Instruct-4bit | 0.56s | mlx |
| **Fastest Inference** | llama-3.1-8b-bf16 | 0.00s | mlx |
| **Memory Efficient** | llama-3.1-8b-bf16 | 0MB | mlx |
| **Highest Throughput** | DialoGPT-medium (HF) | 82.4 tok/s | huggingface |

## Detailed Model Analysis

### llama-3.1-8b-bf16

**Framework:** mlx  
**Size:** 15.0 GB  
**Quantization:** bf16  
**Parameters:** 8,053,063,680 (estimated) if model['parameters'] else 'Unknown'

#### Performance Metrics
- **Load Time:** 0.98 seconds
- **Average Inference:** 0.00 seconds  
- **Throughput:** 0.0 tokens/second
- **Peak Memory:** 0 MB
- **Fine-tuning Capable:** ‚úÖ Yes

### Meta-Llama-3.1-8B-Instruct-4bit

**Framework:** mlx  
**Size:** 4.2 GB  
**Quantization:** 4bit  
**Parameters:** 2,254,857,830 (estimated) if model['parameters'] else 'Unknown'

#### Performance Metrics
- **Load Time:** 0.56 seconds
- **Average Inference:** 0.00 seconds  
- **Throughput:** 0.0 tokens/second
- **Peak Memory:** 2899 MB
- **Fine-tuning Capable:** ‚úÖ Yes

### DialoGPT-medium (HF)

**Framework:** huggingface  
**Size:** 0.7 GB  
**Quantization:** fp16  
**Parameters:** 355,000,000 (estimated) if model['parameters'] else 'Unknown'

#### Performance Metrics
- **Load Time:** 1.65 seconds
- **Average Inference:** 0.61 seconds  
- **Throughput:** 82.4 tokens/second
- **Peak Memory:** 208 MB
- **Fine-tuning Capable:** ‚úÖ Yes


#### Sample Outputs

**Test 1:** Explain the concept of machine learning in simple ...  
**Response:** Machine learning is not a machine.  
**Time:** 0.61s

## Dataset Analysis

Available fine-tuning datasets:

### Quotes Dataset
- **Path:** `examples/data/quotes_train.jsonl`
- **Size:** 20 samples
- **Format:** prompt, response
- **Sample Fields:** `['prompt', 'response']`

### Chat Dataset
- **Path:** `examples/data/train.jsonl`
- **Size:** 80 samples
- **Format:** text
- **Sample Fields:** `['text']`

### Validation Dataset
- **Path:** `examples/data/valid.jsonl`
- **Size:** 15 samples
- **Format:** text
- **Sample Fields:** `['text']`

## Recommendations

- üöÄ For fastest inference: Use llama-3.1-8b-bf16 (0.00s avg)
- üíæ For memory efficiency: Use llama-3.1-8b-bf16 (0MB peak)
- üî• MLX framework shows better performance for Apple Silicon
- üîπ 4-bit quantized models provide good balance of performance and memory usage
- üéØ For fine-tuning: llama-3.1-8b-bf16 offers best estimated performance

## Technical Details

### System Configuration
- **Operating System:** Darwin 25.0.0
- **CPU Cores:** 16
- **Total Memory:** 128 GB
- **Available Memory:** 93 GB

### Framework Status
- **MLX:** ‚úÖ Available
- **LLAMACPP:** ‚ùå Not Available
- **HUGGINGFACE:** ‚úÖ Available
- **MPS:** ‚úÖ Available
- **FLASH_ATTENTION:** ‚úÖ Available

### Model Files Tested

The following 8B parameter models were evaluated:

1. **Llama 3.1 8B (bf16)** - Full precision model (15GB)
2. **Llama 3.1 8B (4-bit)** - Quantized model (4.2GB)  
3. **HuggingFace Comparison Models** - For framework comparison

### Methodology

1. **Loading Test:** Measure time and memory to load each model
2. **Inference Test:** Run 5 standard prompts measuring response time
3. **Memory Analysis:** Track peak memory usage during operations
4. **Fine-tuning Assessment:** Evaluate capability and estimate performance
5. **Quality Sampling:** Capture sample outputs for qualitative analysis

### Limitations

- Fine-tuning tests are estimated rather than actual runs due to time constraints
- Memory measurements include system overhead
- Token counting is estimated based on word count approximations
- Results may vary based on system load and other running processes

---

*Generated by Flow2 Comprehensive 8B Model Benchmark Suite*  
*Timestamp: 2025-06-20T14:09:46.550204*
