//! RIC-TSA Demo: QoE-aware Traffic Steering Application
//! 
//! This example demonstrates the complete RIC-TSA system including:
//! - QoE prediction networks
//! - User group classification
//! - MAC scheduler optimization
//! - A1 policy generation
//! - Knowledge distillation for edge deployment
//! - Sub-millisecond streaming inference

use std::collections::HashMap;
use std::time::Instant;
use tokio::time::{sleep, Duration};

use ran_opt::ric_tsa::*;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    env_logger::init();
    
    println!("ğŸš€ RIC-TSA Demo: QoE-aware Traffic Steering Application");
    println!("====================================================");
    
    // Initialize the RIC-TSA engine
    println!("\nğŸ“¡ Initializing RIC-TSA Engine...");
    let mut ric_engine = RicTsaEngine::new()?;
    
    // Set up network topology
    setup_network_topology(&mut ric_engine).await?;
    
    // Register user equipment
    let ue_contexts = setup_user_equipment(&mut ric_engine).await?;
    
    // Demo 1: Single UE steering decision
    println!("\nğŸ¯ Demo 1: Single UE Traffic Steering Decision");
    println!("------------------------------------------------");
    demo_single_steering_decision(&mut ric_engine, 1).await?;
    
    // Demo 2: Batch processing for multiple UEs
    println!("\nğŸ”„ Demo 2: Batch Processing Multiple UEs");
    println!("----------------------------------------");
    demo_batch_processing(&mut ric_engine, &[1, 2, 3, 4, 5]).await?;
    
    // Demo 3: A1 Policy Generation
    println!("\nğŸ“‹ Demo 3: A1 Policy Generation");
    println!("-------------------------------");
    demo_a1_policy_generation(&mut ric_engine).await?;
    
    // Demo 4: Knowledge Distillation for Edge Deployment
    println!("\nğŸ§  Demo 4: Knowledge Distillation for Edge Deployment");
    println!("-----------------------------------------------------");
    demo_knowledge_distillation(&ric_engine, &ue_contexts).await?;
    
    // Demo 5: Streaming Inference Performance
    println!("\nâš¡ Demo 5: Streaming Inference Performance");
    println!("------------------------------------------");
    demo_streaming_inference(&ric_engine, &ue_contexts).await?;
    
    // Demo 6: Performance Monitoring
    println!("\nğŸ“Š Demo 6: Performance Monitoring");
    println!("----------------------------------");
    demo_performance_monitoring(&ric_engine).await?;
    
    println!("\nâœ… RIC-TSA Demo completed successfully!");
    println!("   Sub-millisecond inference achieved with QoE-aware steering");
    
    Ok(())
}

/// Set up network topology with cells and carriers
async fn setup_network_topology(ric_engine: &mut RicTsaEngine) -> Result<(), Box<dyn std::error::Error>> {
    println!("   Setting up 5G network topology...");
    
    // Add cell carriers for different frequency bands
    let carriers = vec![
        CellCarrier {
            carrier_id: 1,
            band: FrequencyBand::Band1800MHz,
            bandwidth: 20,
            current_load: 45.0,
            max_capacity: 150.0,
            coverage_area: 10.0,
        },
        CellCarrier {
            carrier_id: 2,
            band: FrequencyBand::Band2600MHz,
            bandwidth: 40,
            current_load: 60.0,
            max_capacity: 300.0,
            coverage_area: 5.0,
        },
        CellCarrier {
            carrier_id: 3,
            band: FrequencyBand::Band3500MHz,
            bandwidth: 100,
            current_load: 30.0,
            max_capacity: 1000.0,
            coverage_area: 2.0,
        },
        CellCarrier {
            carrier_id: 4,
            band: FrequencyBand::Band28000MHz,
            bandwidth: 200,
            current_load: 15.0,
            max_capacity: 2000.0,
            coverage_area: 0.5,
        },
        CellCarrier {
            carrier_id: 5,
            band: FrequencyBand::Band700MHz,
            bandwidth: 10,
            current_load: 70.0,
            max_capacity: 50.0,
            coverage_area: 20.0,
        },
    ];
    
    for carrier in carriers {
        println!("   ğŸ“¡ Added cell {} on {:?} band ({}MHz BW, {:.1}% load)", 
            carrier.carrier_id, carrier.band, carrier.bandwidth, carrier.current_load);
        ric_engine.add_cell_carrier(carrier);
    }
    
    println!("   âœ… Network topology configured with 5 cells across different bands");
    Ok(())
}

/// Set up various user equipment with different service requirements
async fn setup_user_equipment(ric_engine: &mut RicTsaEngine) -> Result<HashMap<u64, UEContext>, Box<dyn std::error::Error>> {
    println!("   Registering diverse user equipment...");
    
    let ue_configs = vec![
        // Premium video streaming user
        (1, UserGroup::Premium, ServiceType::VideoStreaming, MobilityPattern::Stationary, 
         QoEMetrics { throughput: 25.0, latency: 15.0, jitter: 3.0, packet_loss: 0.05, 
                     video_quality: 4.5, audio_quality: 4.8, reliability: 99.5, availability: 99.9 }),
        
        // Standard gaming user
        (2, UserGroup::Standard, ServiceType::Gaming, MobilityPattern::Pedestrian,
         QoEMetrics { throughput: 15.0, latency: 10.0, jitter: 2.0, packet_loss: 0.02,
                     video_quality: 4.0, audio_quality: 4.2, reliability: 99.0, availability: 99.5 }),
        
        // Basic voice call user
        (3, UserGroup::Basic, ServiceType::VoiceCall, MobilityPattern::Vehicular,
         QoEMetrics { throughput: 0.064, latency: 50.0, jitter: 10.0, packet_loss: 0.5,
                     video_quality: 3.0, audio_quality: 3.8, reliability: 95.0, availability: 98.0 }),
        
        // IoT sensor device
        (4, UserGroup::IoT, ServiceType::IoTSensor, MobilityPattern::Stationary,
         QoEMetrics { throughput: 0.001, latency: 100.0, jitter: 20.0, packet_loss: 1.0,
                     video_quality: 1.0, audio_quality: 1.0, reliability: 99.9, availability: 99.99 }),
        
        // Emergency AR/VR user
        (5, UserGroup::Emergency, ServiceType::AR_VR, MobilityPattern::HighSpeed,
         QoEMetrics { throughput: 100.0, latency: 5.0, jitter: 1.0, packet_loss: 0.01,
                     video_quality: 5.0, audio_quality: 5.0, reliability: 99.99, availability: 99.999 }),
    ];
    
    let mut ue_contexts = HashMap::new();
    
    for (ue_id, user_group, service_type, mobility, current_qoe) in ue_configs {
        let device_caps = match user_group {
            UserGroup::Premium | UserGroup::Emergency => DeviceCapabilities {
                supported_bands: vec![FrequencyBand::Band1800MHz, FrequencyBand::Band2600MHz, 
                                    FrequencyBand::Band3500MHz, FrequencyBand::Band28000MHz],
                max_mimo_layers: 4,
                ca_support: true,
                dual_connectivity: true,
            },
            UserGroup::Standard => DeviceCapabilities {
                supported_bands: vec![FrequencyBand::Band1800MHz, FrequencyBand::Band2600MHz],
                max_mimo_layers: 2,
                ca_support: true,
                dual_connectivity: false,
            },
            _ => DeviceCapabilities {
                supported_bands: vec![FrequencyBand::Band1800MHz, FrequencyBand::Band700MHz],
                max_mimo_layers: 1,
                ca_support: false,
                dual_connectivity: false,
            },
        };
        
        let service_reqs = match service_type {
            ServiceType::AR_VR => ServiceRequirements {
                min_throughput: 50.0, max_latency: 10.0, max_jitter: 2.0, 
                max_packet_loss: 0.01, priority: 250,
            },
            ServiceType::Gaming => ServiceRequirements {
                min_throughput: 10.0, max_latency: 20.0, max_jitter: 5.0,
                max_packet_loss: 0.1, priority: 200,
            },
            ServiceType::VideoStreaming => ServiceRequirements {
                min_throughput: 5.0, max_latency: 100.0, max_jitter: 20.0,
                max_packet_loss: 1.0, priority: 150,
            },
            ServiceType::VoiceCall => ServiceRequirements {
                min_throughput: 0.064, max_latency: 150.0, max_jitter: 30.0,
                max_packet_loss: 3.0, priority: 180,
            },
            ServiceType::Emergency => ServiceRequirements {
                min_throughput: 1.0, max_latency: 50.0, max_jitter: 10.0,
                max_packet_loss: 0.1, priority: 255,
            },
            _ => ServiceRequirements {
                min_throughput: 0.001, max_latency: 1000.0, max_jitter: 100.0,
                max_packet_loss: 10.0, priority: 50,
            },
        };
        
        let ue_context = UEContext {
            ue_id,
            user_group: user_group.clone(),
            service_type: service_type.clone(),
            current_qoe,
            location: (40.7128 + (ue_id as f64) * 0.01, -74.0060 + (ue_id as f64) * 0.01),
            mobility_pattern: mobility,
            device_capabilities: device_caps,
            service_requirements: service_reqs,
        };
        
        println!("   ğŸ“± Registered UE {} - {:?} {:?} user with {:?} mobility", 
            ue_id, user_group, service_type, mobility);
        
        ric_engine.register_ue(ue_context.clone());
        ue_contexts.insert(ue_id, ue_context);
    }
    
    println!("   âœ… Registered 5 diverse UE devices across different service types");
    Ok(ue_contexts)
}

/// Demonstrate single UE steering decision with timing analysis
async fn demo_single_steering_decision(ric_engine: &mut RicTsaEngine, ue_id: u64) -> Result<(), Box<dyn std::error::Error>> {
    println!("   Making steering decision for UE {}...", ue_id);
    
    let start_time = Instant::now();
    let decision = ric_engine.make_steering_decision(ue_id).await?;
    let decision_time = start_time.elapsed();
    
    println!("   âš¡ Decision made in {:.3}ms (target: <1ms)", decision_time.as_secs_f64() * 1000.0);
    println!("   ğŸ¯ Target: Cell {} on {:?} band", decision.target_cell, decision.target_band);
    println!("   ğŸ“Š Resource allocation: {} PRBs, MCS {}, {} MIMO layers", 
        decision.resource_allocation.prb_allocation.len(),
        decision.resource_allocation.mcs_index,
        decision.resource_allocation.mimo_layers);
    println!("   ğŸ–ï¸  Confidence: {:.2}%, Priority: {}", 
        decision.confidence * 100.0,
        decision.resource_allocation.scheduling_priority);
    
    // Verify sub-millisecond performance
    if decision_time.as_millis() < 1 {
        println!("   âœ… Sub-millisecond performance achieved!");
    } else {
        println!("   âš ï¸  Decision time exceeded 1ms target");
    }
    
    Ok(())
}

/// Demonstrate batch processing for multiple UEs
async fn demo_batch_processing(ric_engine: &mut RicTsaEngine, ue_ids: &[u64]) -> Result<(), Box<dyn std::error::Error>> {
    println!("   Processing batch of {} UEs...", ue_ids.len());
    
    let start_time = Instant::now();
    let decisions = ric_engine.batch_steering_decisions(ue_ids).await?;
    let batch_time = start_time.elapsed();
    
    println!("   âš¡ Batch processed in {:.3}ms ({:.3}ms per UE)", 
        batch_time.as_secs_f64() * 1000.0,
        batch_time.as_secs_f64() * 1000.0 / ue_ids.len() as f64);
    
    for decision in &decisions {
        println!("   ğŸ“‹ UE {}: Cell {} â†’ {} PRBs, {:.0}% confidence", 
            decision.ue_id,
            decision.target_cell,
            decision.resource_allocation.prb_allocation.len(),
            decision.confidence * 100.0);
    }
    
    // Calculate throughput
    let throughput = ue_ids.len() as f64 / batch_time.as_secs_f64();
    println!("   ğŸ“ˆ Throughput: {:.0} decisions/second", throughput);
    
    if throughput > 1000.0 {
        println!("   âœ… High-throughput batch processing achieved!");
    }
    
    Ok(())
}

/// Demonstrate A1 policy generation
async fn demo_a1_policy_generation(ric_engine: &mut RicTsaEngine) -> Result<(), Box<dyn std::error::Error>> {
    println!("   Generating A1 policies from steering decisions...");
    
    // Make a steering decision first
    let decision = ric_engine.make_steering_decision(1).await?;
    
    // Generate A1 policy
    let start_time = Instant::now();
    let policy = ric_engine.generate_a1_policy(&decision).await?;
    let policy_time = start_time.elapsed();
    
    println!("   âš¡ A1 policy generated in {:.3}ms", policy_time.as_secs_f64() * 1000.0);
    println!("   ğŸ“‹ Policy ID: {}", policy.policy_id);
    println!("   ğŸ·ï¸  Policy Type: {:?}", policy.policy_type);
    println!("   ğŸ¯ Scope: {} cells, {} user groups", 
        policy.scope.cell_ids.len(),
        policy.scope.user_groups.len());
    println!("   âš–ï¸  Priority: {}, Enforcement: {:?}", 
        policy.priority,
        policy.enforcement_mode);
    println!("   ğŸ“ Conditions: {}, Actions: {}", 
        policy.conditions.len(),
        policy.actions.len());
    println!("   ğŸ–ï¸  Confidence: {:.1}%", policy.metadata.confidence * 100.0);
    
    // Display key policy parameters
    println!("   ğŸ“Š QoE Thresholds:");
    println!("      ğŸ“¡ Min Throughput: {:.1} Mbps", policy.parameters.qoe_thresholds.min_throughput);
    println!("      â±ï¸  Max Latency: {:.1} ms", policy.parameters.qoe_thresholds.max_latency);
    println!("      ğŸ“¶ Min Reliability: {:.1}%", policy.parameters.qoe_thresholds.min_reliability);
    
    Ok(())
}

/// Demonstrate knowledge distillation for edge deployment
async fn demo_knowledge_distillation(ric_engine: &RicTsaEngine, ue_contexts: &HashMap<u64, UEContext>) -> Result<(), Box<dyn std::error::Error>> {
    println!("   Creating teacher models and performing knowledge distillation...");
    
    // Create teacher models
    let teacher_qoe = std::sync::Arc::new(QoEPredictor::new()?);
    let teacher_classifier = std::sync::Arc::new(UserClassifier::new()?);
    let teacher_scheduler = std::sync::Arc::new(MacScheduler::new()?);
    
    // Initialize knowledge distillation
    let distillation = KnowledgeDistillation::new(
        teacher_qoe,
        teacher_classifier,
        teacher_scheduler,
    )?;
    
    println!("   ğŸ“š Collecting training data from teacher models...");
    let contexts: Vec<UEContext> = ue_contexts.values().cloned().collect();
    distillation.collect_training_data(&contexts).await?;
    
    println!("   ğŸ§  Training student models via knowledge distillation...");
    let train_start = Instant::now();
    distillation.train_student_models().await?;
    let train_time = train_start.elapsed();
    
    println!("   âš¡ Training completed in {:.2}s", train_time.as_secs_f64());
    
    // Create edge model
    println!("   ğŸ“¦ Creating compressed edge model...");
    let edge_model = distillation.create_edge_model().await?;
    let metadata = edge_model.get_metadata();
    
    println!("   ğŸ“Š Edge Model Metrics:");
    println!("      ğŸ—œï¸  Compression Ratio: {:.1}x smaller", 1.0 / metadata.compression_ratio);
    println!("      ğŸ“ Model Size: {:.1} MB", metadata.model_size_mb);
    println!("      âš¡ Target Inference Time: {:.2} ms", metadata.inference_time_ms);
    println!("      ğŸ¯ Accuracy Retention: {:.1}%", metadata.accuracy_retention * 100.0);
    
    // Validate student models
    println!("   ğŸ§ª Validating student model performance...");
    let validation = distillation.validate_student_models(&contexts).await?;
    
    println!("   ğŸ“ˆ Validation Results:");
    println!("      ğŸ¯ QoE Accuracy: {:.1}%", validation.qoe_accuracy * 100.0);
    println!("      ğŸ‘¥ Classification Accuracy: {:.1}%", validation.classification_accuracy * 100.0);
    println!("      ğŸ“‹ Scheduling Accuracy: {:.1}%", validation.scheduling_accuracy * 100.0);
    println!("      âš¡ Avg Inference Time: {:.2} ms", validation.avg_inference_time_ms);
    println!("      âœ… Meets Latency Target: {}", validation.meets_latency_target);
    
    if validation.meets_latency_target {
        println!("   âœ… Edge model ready for deployment!");
    }
    
    Ok(())
}

/// Demonstrate streaming inference performance
async fn demo_streaming_inference(ric_engine: &RicTsaEngine, ue_contexts: &HashMap<u64, UEContext>) -> Result<(), Box<dyn std::error::Error>> {
    println!("   Setting up streaming inference engine...");
    
    let streaming_engine = StreamingInferenceEngine::new()?;
    
    // Simulate high-throughput streaming workload
    println!("   ğŸŒŠ Simulating high-throughput streaming workload...");
    
    let mut total_processing_time = Duration::new(0, 0);
    let mut total_requests = 0;
    
    for batch_round in 1..=5 {
        let batch_size = 10 + batch_round * 5; // Increasing batch sizes
        let ue_ids: Vec<u64> = (1..=batch_size as u64).collect();
        
        let start_time = Instant::now();
        let results = streaming_engine.process_batch(&ue_ids, ue_contexts).await;
        let batch_time = start_time.elapsed();
        
        match results {
            Ok(batch_results) => {
                total_processing_time += batch_time;
                total_requests += batch_results.len();
                
                let avg_time_per_request = batch_time.as_secs_f64() * 1000.0 / batch_results.len() as f64;
                
                println!("   ğŸ“Š Batch {}: {} UEs in {:.2}ms ({:.3}ms/UE)", 
                    batch_round, batch_results.len(), 
                    batch_time.as_secs_f64() * 1000.0, avg_time_per_request);
                
                // Show first few results
                for (i, (ue_id, result)) in batch_results.iter().take(3).enumerate() {
                    println!("      ğŸ“± UE {}: Cell {} ({:.0}% conf, {:.0}Î¼s)", 
                        ue_id, result.target_cell, 
                        result.confidence * 100.0,
                        result.processing_time_ns as f64 / 1000.0);
                }
                
                if batch_results.len() > 3 {
                    println!("      ... and {} more results", batch_results.len() - 3);
                }
            }
            Err(_e) => {
                println!("   âš ï¸  Batch {} failed (expected without registered edge models)", batch_round);
                // This is expected since we don't have actual edge models registered
                continue;
            }
        }
        
        // Small delay between batches
        sleep(Duration::from_millis(10)).await;
    }
    
    if total_requests > 0 {
        let overall_throughput = total_requests as f64 / total_processing_time.as_secs_f64();
        let avg_latency = total_processing_time.as_secs_f64() * 1000.0 / total_requests as f64;
        
        println!("   ğŸ“ˆ Overall Streaming Performance:");
        println!("      ğŸ”„ Total Requests: {}", total_requests);
        println!("      âš¡ Throughput: {:.0} req/sec", overall_throughput);
        println!("      â±ï¸  Avg Latency: {:.2} ms", avg_latency);
        
        if avg_latency < 1.0 {
            println!("   âœ… Sub-millisecond streaming achieved!");
        }
    }
    
    // Show performance metrics
    let perf_metrics = streaming_engine.get_performance_metrics().await;
    println!("   ğŸ“Š Streaming Engine Metrics:");
    println!("      ğŸ“¥ Total Requests: {}", perf_metrics.total_requests);
    println!("      âš¡ Avg Processing Time: {:.0} Î¼s", perf_metrics.avg_processing_time_ns as f64 / 1000.0);
    println!("      ğŸ“ˆ Throughput: {:.1} req/sec", perf_metrics.throughput_requests_per_sec);
    println!("      ğŸ¯ Batch Utilization: {:.1}%", perf_metrics.batch_utilization * 100.0);
    
    Ok(())
}

/// Demonstrate performance monitoring
async fn demo_performance_monitoring(ric_engine: &RicTsaEngine) -> Result<(), Box<dyn std::error::Error>> {
    println!("   Analyzing RIC-TSA performance metrics...");
    
    let metrics = ric_engine.get_performance_metrics();
    
    println!("   ğŸ“Š RIC-TSA Performance Summary:");
    println!("      âš¡ Avg Inference Time: {:.2} ms", metrics.inference_time_ms);
    println!("      ğŸ”„ Decision Throughput: {:.0} decisions/sec", metrics.throughput_decisions_per_sec);
    println!("      ğŸ“ˆ QoE Improvement: {:.1}%", metrics.qoe_improvement_ratio * 100.0);
    println!("      ğŸ¯ Handover Success Rate: {:.1}%", metrics.handover_success_rate * 100.0);
    println!("      ğŸ­ Resource Utilization: {:.1}%", metrics.resource_utilization * 100.0);
    println!("      ğŸ–ï¸  Prediction Accuracy: {:.1}%", metrics.prediction_accuracy * 100.0);
    
    // Performance assessment
    println!("   ğŸ¯ Performance Assessment:");
    
    if metrics.inference_time_ms < 1.0 {
        println!("      âœ… Sub-millisecond inference target met");
    } else {
        println!("      âš ï¸  Inference time above 1ms target");
    }
    
    if metrics.qoe_improvement_ratio > 0.1 {
        println!("      âœ… Significant QoE improvement achieved");
    } else {
        println!("      âš ï¸  QoE improvement below 10% target");
    }
    
    if metrics.handover_success_rate > 0.95 {
        println!("      âœ… High handover success rate");
    } else {
        println!("      âš ï¸  Handover success rate below 95%");
    }
    
    if metrics.resource_utilization > 0.7 && metrics.resource_utilization < 0.9 {
        println!("      âœ… Optimal resource utilization");
    } else {
        println!("      âš ï¸  Resource utilization outside optimal range");
    }
    
    // Recommendations
    println!("   ğŸ’¡ Optimization Recommendations:");
    if metrics.inference_time_ms > 0.5 {
        println!("      ğŸš€ Consider additional model compression for edge deployment");
    }
    if metrics.resource_utilization < 0.7 {
        println!("      ğŸ“ˆ Increase resource allocation aggressiveness");
    }
    if metrics.handover_success_rate < 0.9 {
        println!("      ğŸ”§ Tune handover decision thresholds");
    }
    if metrics.qoe_improvement_ratio < 0.05 {
        println!("      ğŸ¯ Refine QoE prediction model accuracy");
    }
    
    Ok(())
}