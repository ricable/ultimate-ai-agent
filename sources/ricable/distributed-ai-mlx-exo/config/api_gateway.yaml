# API Gateway Configuration - Phase 3
# Enhanced distributed API server settings for Apple Silicon cluster

server:
  host: "0.0.0.0"
  port: 8000
  workers: 1
  log_level: "info"
  reload: false
  trusted_hosts: ["*"]
  cors_origins: ["*"]
  max_requests: 1000
  request_timeout: 300

# Authentication settings
auth:
  enabled: true
  jwt_secret: null  # Will be auto-generated if null
  create_admin_key: true
  max_key_age_days: 365
  max_failed_attempts: 5
  lockout_duration: 300

# Rate limiting settings
rate_limiting:
  enabled: true
  adaptive: true
  default_tier: "standard"
  cleanup_interval: 3600
  rate_limit_window: 60
  
  # Rate limit tiers
  tiers:
    free:
      requests_per_minute: 60
      requests_per_hour: 1000
      concurrent_requests: 2
      tokens_per_minute: 10000
    standard:
      requests_per_minute: 300
      requests_per_hour: 10000
      concurrent_requests: 10
      tokens_per_minute: 50000
    premium:
      requests_per_minute: 1000
      requests_per_hour: 50000
      concurrent_requests: 50
      tokens_per_minute: 200000
    enterprise:
      requests_per_minute: 5000
      concurrent_requests: 200
      tokens_per_minute: 1000000

# Load balancing settings
load_balancing:
  enabled: true
  strategy: "resource_aware"  # round_robin, least_connections, weighted_round_robin, resource_aware, consistent_hashing
  health_check_interval: 30
  max_retries: 3
  request_timeout: 300

# Cluster nodes configuration (Apple Silicon nodes)
cluster_nodes:
  - node_id: "mac-studio-m1-1"
    host: "10.0.1.10"
    port: 8001
    weight: 1.0
    max_connections: 100
    capabilities: ["inference", "training"]
    active_models: ["llama-7b", "llama-13b", "mistral-7b"]
    specs:
      chip: "M1 Max"
      memory_gb: 64
      gpu_cores: 32
      cpu_cores: 10
  
  - node_id: "mac-studio-m1-2"
    host: "10.0.1.11"
    port: 8001
    weight: 1.0
    max_connections: 100
    capabilities: ["inference", "training"]
    active_models: ["llama-7b", "llama-13b", "mistral-7b"]
    specs:
      chip: "M1 Max"
      memory_gb: 64
      gpu_cores: 32
      cpu_cores: 10
  
  - node_id: "mac-studio-m2"
    host: "10.0.1.12"
    port: 8001
    weight: 0.8
    max_connections: 80
    capabilities: ["inference"]
    active_models: ["llama-7b", "mistral-7b"]
    specs:
      chip: "M2 Max"
      memory_gb: 32
      gpu_cores: 30
      cpu_cores: 12
  
# Default models to load on startup
default_models:
  - name: "llama-7b"
    architecture: "llama"
    num_layers: 32
    hidden_size: 4096
    num_attention_heads: 32
    vocab_size: 32000
    max_sequence_length: 2048
    quantization: "4bit"
    memory_requirements_gb: 6
    
  - name: "llama-13b"
    architecture: "llama"
    num_layers: 40
    hidden_size: 5120
    num_attention_heads: 40
    vocab_size: 32000
    max_sequence_length: 2048
    quantization: "4bit"
    memory_requirements_gb: 12
    
  - name: "llama-30b"
    architecture: "llama"
    num_layers: 60
    hidden_size: 6656
    num_attention_heads: 52
    vocab_size: 32000
    max_sequence_length: 2048
    quantization: "4bit"
    memory_requirements_gb: 28
    
  - name: "llama-70b"
    architecture: "llama"
    num_layers: 80
    hidden_size: 8192
    num_attention_heads: 64
    vocab_size: 32000
    max_sequence_length: 2048
    quantization: "4bit"
    memory_requirements_gb: 65
    
  - name: "mistral-7b"
    architecture: "mistral"
    num_layers: 32
    hidden_size: 4096
    num_attention_heads: 32
    vocab_size: 32000
    max_sequence_length: 8192
    quantization: "4bit"
    memory_requirements_gb: 6

# Monitoring and metrics settings
monitoring:
  metrics_enabled: true
  health_check_endpoint: true
  detailed_logging: true
  prometheus_metrics: true
  prometheus_port: 9090
  log_requests: true
  log_responses: false  # Set to true for debugging
  
  # Performance thresholds for alerts
  thresholds:
    max_response_time_ms: 5000
    max_error_rate_percent: 5
    max_memory_usage_percent: 90
    max_cpu_usage_percent: 80

# Security settings
security:
  request_timeout: 300
  max_request_size: "10MB"
  enable_ssl: false
  ssl_cert_path: null
  ssl_key_path: null
  allowed_ips: []  # Empty means all IPs allowed
  blocked_ips: []
  
  # API key settings
  api_key_header: "Authorization"
  api_key_prefix: "Bearer"
  
  # CORS settings
  cors_allow_origins: ["*"]
  cors_allow_methods: ["GET", "POST", "PUT", "DELETE", "OPTIONS"]
  cors_allow_headers: ["*"]
  cors_expose_headers: ["X-Request-ID", "X-Response-Time"]

# Storage and caching settings
storage:
  model_cache_dir: "./models"
  temp_dir: "./tmp"
  logs_dir: "./logs"
  data_dir: "./data"
  
  # Model repository settings
  model_repository:
    type: "local"  # local, s3, gcs, azure
    base_path: "./models"
    auto_download: true
    cache_models: true
    max_cache_size_gb: 500

# MLX and Exo integration settings
mlx:
  optimization_level: 2
  enable_quantization: true
  default_quantization_bits: 4
  memory_pool_size_gb: 16
  lazy_evaluation: true
  
exo:
  discovery_method: "mdns"  # mdns, static, kubernetes
  discovery_timeout: 30
  heartbeat_interval: 10
  partition_strategy: "ring_memory_weighted"
  enable_compression: true

# Logging configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file_logging: true
  console_logging: true
  max_log_size_mb: 100
  backup_count: 5
  
  # Component-specific log levels
  component_levels:
    "load_balancer": "INFO"
    "auth": "INFO"
    "rate_limiter": "INFO"
    "inference_engine": "INFO"
    "memory_manager": "DEBUG"

# Development settings
development:
  auto_reload: false
  debug_mode: false
  mock_inference: false
  enable_profiling: false
  
# Production settings
production:
  worker_processes: 3
  max_requests_per_worker: 10000
  worker_timeout: 300
  preload_models: true
  enable_health_checks: true
  graceful_shutdown_timeout: 30

# Backup and recovery
backup:
  enabled: false
  interval_hours: 24
  retention_days: 7
  backup_models: false
  backup_configs: true
  backup_logs: false